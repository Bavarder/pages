<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><style>:root{--accent-color:#E66100}</style><link rel=stylesheet type=text/css href=/css/style.css><title>Bavarder</title><meta name=description content><meta name=keywords content><meta property="og:url" content="http://bavarder.codeberg.page/help/local/"><meta property="og:type" content="website"><meta property="og:title" content="Bavarder"><meta property="og:description" content><meta property="og:image" content="http://bavarder.codeberg.page/"><meta property="og:image:secure_url" content="http://bavarder.codeberg.page/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Bavarder"><meta name=twitter:description content><meta property="twitter:domain" content="http://bavarder.codeberg.page/help/local/"><meta property="twitter:url" content="http://bavarder.codeberg.page/help/local/"><meta name=twitter:image content="http://bavarder.codeberg.page/"><link rel=canonical href=http://bavarder.codeberg.page/help/local/><link rel="shortcut icon" type=image/png href=/assets/favicon.png><link rel=apple-touch-icon type=image/png href=/assets/apple-touch-icon.png><script defer data-domain=bavarder.codeberg.page src=/js/script.js></script></head><body><header class=site-header><h1 id=logo><a href=http://bavarder.codeberg.page/></a></h1></header><div class=container><h1 id=local-models>Local models</h1><p>You can run model locally with tools like <a href=https://github.com/lm-sys/FastChat>FastChat</a> or <a href=https://github.com/go-skynet/LocalAI>LocalAI</a> which provide a OpenAI-API compatible API.</p><h2 id=examples>Examples</h2><h3 id=fastchat>FastChat</h3><p>Full documentation is available <a href=https://github.com/lm-sys/FastChat/blob/main/README.md>here</a> but for an easy setup of vicuna you can follow this instructions.</p><ul><li><p>First, install <a href=https://github.com/lm-sys/FastChat>FastChat</a> with pip by running <code>pip install fschat</code></p></li><li><p>After, you can first launch the controller</p></li></ul><pre tabindex=0><code>python3 -m fastchat.serve.controller
</code></pre><ul><li>Then, launch the model worker(s), which will automatically download the weights from a HuggingFace <a href=https://huggingface.co/lmsys/fastchat-t5-3b-v1.0>repository</a>.</li></ul><pre tabindex=0><code>python3 -m fastchat.serve.model_worker --model-name &#39;vicuna-7b-v1.1&#39; --model-path lmsys/fastchat-t5-3b-v1.0
</code></pre><ul><li>Finally, launch the RESTful API server</li></ul><pre tabindex=0><code>python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
</code></pre><p>Now, you can use the API server by following <a href=#custom-api-url>Custom API Url</a> and <a href=#custom-model>Custom Model</a></p><h2 id=custom-api-url>Custom API URL</h2><p>Maybe you are using a custom endpoint which is providing an OpenAI-API compatible API. For using a custom endpoint, you just need to provide the base url in the preferences. For example, if you are using <a href=https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md>FastChat</a>, you need to put <code>http://localhost:8000/v1</code> in Preferences > Local Model > API URL.</p><h2 id=custom-model>Custom Model</h2><p>If you want to use a model of an OpenAI-API compatible API or from the OpenAI API which isn&rsquo;t available in Bavarder (available: <code>gpt-3.5-turbo</code>, <code>gpt-4</code>, <code>text-davinci-003</code>), you can use the provider callled OpenAI Custom Model and set Model to the model you want to use. For example, if you want to use <code>gpt-4-32k</code> you just have to set Model to <code>gpt-4-32k</code></p><p class=dialog-buttons><a href=/ class=inline-button>Go Home</a> <a href=https://codeberg.org/Bavarder/pages/issues>File an issue</a></p></div><footer class=site-footer><p>&copy; Bavarder, 2023</p><p><a href=https://codeberg.org/Bavarder/pages>Website source</a></p></footer></body></html>